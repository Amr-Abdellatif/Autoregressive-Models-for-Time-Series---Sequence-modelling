{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Autoregressive models ?\n",
    "\n",
    "### Do you remeber the autoencoder ?\n",
    "\n",
    "The autoencoding approach has been very successful for images, signals, and even fully connected\n",
    "models with tabular data. But what if our data is a sequence problem? Especially if our data is in a\n",
    "language represented by discrete tokens, it’s hard to add meaningful noise to things like a letter or\n",
    "word. Instead, we can use an autoregressive model, which is an approach specifically designed for\n",
    "time-series problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\git-hub\\Autoregressive Models for Time-Series & Sequence modelling\\utils.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from utils import View\n",
    "from utils import train_network\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "# from idlmam import train_network, Flatten, View, weight_reset,  set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "all_data = [] \n",
    "resp = urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\")\n",
    "shakespear_100k = resp.read()\n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  36\n",
      "Total Characters: 99993\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {}\n",
    "for char in shakespear_100k:\n",
    "    if char not in vocab2indx:\n",
    "        vocab2indx[char] = len(vocab2indx)\n",
    "indx2vocab = {}\n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k\n",
    "print(\"Vocab Size: \", len(vocab2indx))\n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an autoregressive dataset from one single, long, source sequence by breaking it up into \"chunks\". \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, MAX_CHUNK=500):\n",
    "        \"\"\"\n",
    "        large_string: the original long source sequence that chunks will be extracted from\n",
    "        MAX_CHUNK: the maximum allowed size of any chunk. \n",
    "        \"\"\"\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "    def __len__(self):\n",
    "        #The number of items is the number of characters divided by chunk size\n",
    "        return (len(self.doc)-1) // self.MAX_CHUNK\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Compute the starting position for the idx'th chunk\n",
    "        start = idx*self.MAX_CHUNK\n",
    "        #Grab the input sub-string\n",
    "        sub_string = self.doc[start:start+self.MAX_CHUNK]\n",
    "        #convert the sub-string into integers based on our vocab\n",
    "        x = [vocab2indx[c] for c in sub_string]\n",
    "        \n",
    "        #grab the label sub-string by shifting over by 1\n",
    "        sub_string = self.doc[start+1:start+self.MAX_CHUNK+1]\n",
    "        #convert the label sub-string into integers based on our vocab\n",
    "        y = [vocab2indx[c] for c in sub_string]\n",
    "        #convert the \n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressive(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "        super(AutoRegressive, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(embd_size, hidden_size)] + \n",
    "                                     [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.pred_class = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),# (B, *, D)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size), # (B, *, D)\n",
    "            nn.Linear(hidden_size, num_embeddings) #(B, *. D) -> B(B, *, VocabSize)\n",
    "        )\n",
    "        \n",
    "    def initHiddenStates(self, B):\n",
    "        \"\"\"\n",
    "        Creates an initial hidden state list for the RNN layers. \n",
    "        \n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\"\n",
    "        return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "    def step(self, x_in, h_prevs=None):\n",
    "        \"\"\"\n",
    "        x_in: the input for this current time step and has shape (B) if the values need \n",
    "            to be embedded, and (B, D) if they have alreayd been embedded. \n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape (B, self.hidden_size) for each \n",
    "            layer in the network. These contain the current hidden state of the RNN layers and \n",
    "            will be updated by this call. \n",
    "        \"\"\"\n",
    "        #Prep all three arguments to be in the final form\n",
    "        if len(x_in.shape) == 1: #(B), we need to embed it\n",
    "            x_in = self.embd(x_in) #now (B, D)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "        \n",
    "        #Process the input \n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l]\n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h\n",
    "            x_in = h\n",
    "        #Make predictions about the token\n",
    "        return self.pred_class(x_in)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            last_activations.append(self.step(x_in, h_prevs))\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        return last_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData = AutoRegressiveDataset(shakespear_100k, MAX_CHUNK=250)\n",
    "autoReg_loader = DataLoader(autoRegData, batch_size=128, shuffle=True)\n",
    "\n",
    "autoReg_model = AutoRegressive(len(vocab2indx), 32, 128, layers=2)\n",
    "autoReg_model = autoReg_model.to(device)\n",
    "\n",
    "for p in autoReg_model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y):\n",
    "    \"\"\"\n",
    "    x: output with shape (B, T, V)\n",
    "    y: labels with shape (B, T)\n",
    "    \n",
    "    \"\"\"\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    T = x.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):#for every item in the sequence\n",
    "        loss += cel(x[:,t,:], y[:,t]) #Compute the sum of prediction errors\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 50/50 [01:15<00:00,  1.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>total time</th>\n",
       "      <th>train loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.749971</td>\n",
       "      <td>757.020416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.376275</td>\n",
       "      <td>742.323990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.062328</td>\n",
       "      <td>723.743301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.682607</td>\n",
       "      <td>704.701569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.317533</td>\n",
       "      <td>684.282745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9.941523</td>\n",
       "      <td>664.779068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>11.367311</td>\n",
       "      <td>647.298874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>12.898011</td>\n",
       "      <td>633.062164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>14.341451</td>\n",
       "      <td>617.980743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>15.834649</td>\n",
       "      <td>605.811142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>17.264294</td>\n",
       "      <td>593.637207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>18.687119</td>\n",
       "      <td>583.262939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>20.169816</td>\n",
       "      <td>576.023926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>21.636370</td>\n",
       "      <td>564.348648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>23.124583</td>\n",
       "      <td>554.776276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>24.554048</td>\n",
       "      <td>550.050537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>26.077714</td>\n",
       "      <td>541.506027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>27.489329</td>\n",
       "      <td>532.600937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>28.981264</td>\n",
       "      <td>528.685669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>30.513312</td>\n",
       "      <td>520.500687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>32.059638</td>\n",
       "      <td>514.335121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>33.642700</td>\n",
       "      <td>507.212646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>35.223979</td>\n",
       "      <td>501.931442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>36.741058</td>\n",
       "      <td>497.571083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>38.188772</td>\n",
       "      <td>491.789024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>39.678411</td>\n",
       "      <td>489.149910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>41.157748</td>\n",
       "      <td>484.946861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>42.643928</td>\n",
       "      <td>481.355118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>44.147364</td>\n",
       "      <td>474.955803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>45.585846</td>\n",
       "      <td>470.806984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>46.997131</td>\n",
       "      <td>468.679428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>48.530564</td>\n",
       "      <td>462.233337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>50.084733</td>\n",
       "      <td>459.651398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>51.511599</td>\n",
       "      <td>456.186752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>52.978534</td>\n",
       "      <td>453.253006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>54.470525</td>\n",
       "      <td>451.269081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>55.973114</td>\n",
       "      <td>445.302681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>57.403728</td>\n",
       "      <td>441.842865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>58.832701</td>\n",
       "      <td>437.095161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>60.354315</td>\n",
       "      <td>438.423965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>61.755232</td>\n",
       "      <td>434.754173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>63.177284</td>\n",
       "      <td>430.490509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>64.685955</td>\n",
       "      <td>425.687851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>66.158175</td>\n",
       "      <td>427.068886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>67.589823</td>\n",
       "      <td>425.710640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>69.051367</td>\n",
       "      <td>420.671677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>70.505694</td>\n",
       "      <td>417.239182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>71.926392</td>\n",
       "      <td>415.490494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>73.337694</td>\n",
       "      <td>413.068932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>74.821074</td>\n",
       "      <td>409.502899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  total time  train loss\n",
       "0       0    1.749971  757.020416\n",
       "1       1    3.376275  742.323990\n",
       "2       2    5.062328  723.743301\n",
       "3       3    6.682607  704.701569\n",
       "4       4    8.317533  684.282745\n",
       "5       5    9.941523  664.779068\n",
       "6       6   11.367311  647.298874\n",
       "7       7   12.898011  633.062164\n",
       "8       8   14.341451  617.980743\n",
       "9       9   15.834649  605.811142\n",
       "10     10   17.264294  593.637207\n",
       "11     11   18.687119  583.262939\n",
       "12     12   20.169816  576.023926\n",
       "13     13   21.636370  564.348648\n",
       "14     14   23.124583  554.776276\n",
       "15     15   24.554048  550.050537\n",
       "16     16   26.077714  541.506027\n",
       "17     17   27.489329  532.600937\n",
       "18     18   28.981264  528.685669\n",
       "19     19   30.513312  520.500687\n",
       "20     20   32.059638  514.335121\n",
       "21     21   33.642700  507.212646\n",
       "22     22   35.223979  501.931442\n",
       "23     23   36.741058  497.571083\n",
       "24     24   38.188772  491.789024\n",
       "25     25   39.678411  489.149910\n",
       "26     26   41.157748  484.946861\n",
       "27     27   42.643928  481.355118\n",
       "28     28   44.147364  474.955803\n",
       "29     29   45.585846  470.806984\n",
       "30     30   46.997131  468.679428\n",
       "31     31   48.530564  462.233337\n",
       "32     32   50.084733  459.651398\n",
       "33     33   51.511599  456.186752\n",
       "34     34   52.978534  453.253006\n",
       "35     35   54.470525  451.269081\n",
       "36     36   55.973114  445.302681\n",
       "37     37   57.403728  441.842865\n",
       "38     38   58.832701  437.095161\n",
       "39     39   60.354315  438.423965\n",
       "40     40   61.755232  434.754173\n",
       "41     41   63.177284  430.490509\n",
       "42     42   64.685955  425.687851\n",
       "43     43   66.158175  427.068886\n",
       "44     44   67.589823  425.710640\n",
       "45     45   69.051367  420.671677\n",
       "46     46   70.505694  417.239182\n",
       "47     47   71.926392  415.490494\n",
       "48     48   73.337694  413.068932\n",
       "49     49   74.821074  409.502899"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_network(autoReg_model, CrossEntLossTime, autoReg_loader, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoReg_model = autoReg_model.eval()\n",
    "sampling = torch.zeros((1, 500), dtype=torch.int64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"EMILIA:\".lower()\n",
    "cur_len = len(seed)\n",
    "sampling[0,0:cur_len] = torch.tensor([vocab2indx[x] for x in seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:52<00:00,  9.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(cur_len, sampling.size(1))):\n",
    "    with torch.no_grad():\n",
    "        h = autoReg_model(sampling[:,0:i]) #process all the previous items\n",
    "        h = h[:,-1,:] #Grab the last time step\n",
    "        h = F.softmax(h, dim=1) #make probabilities\n",
    "        next_tokens = torch.multinomial(h, 1) #sample the next prediction\n",
    "        sampling[:,i] = next_tokens #set the next prediction\n",
    "        #increase the length by one\n",
    "        cur_len += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emilia:\n",
      "a felliabir ullings: ahait ocessices fulled\n",
      "and thy suybir printries it love givented mermight\n",
      "truse indeed denity emass kingdariand\n",
      "dukess shear pary my him.\n",
      "\n",
      "craindus:\n",
      "faind up mountiof they tov joingty,\n",
      "couds poof\n",
      "the many shund but's hour the will:\n",
      "as for ga hat dewardy.\n",
      "\n",
      "suchiress:\n",
      "feed fate,\n",
      "but i hert caod:\n",
      "tentiniady, good inetsulf,\n",
      "that love: that age,yu lord, let of their of here?\n",
      "\n",
      "churublet:\n",
      "i camen you have oft could breenute of foierduphter you bruve hears undands ronielit a\n"
     ]
    }
   ],
   "source": [
    "s = [indx2vocab[x] for x in sampling.cpu().numpy().flatten()]\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:53<00:00,  9.16it/s]\n"
     ]
    }
   ],
   "source": [
    "cur_len = len(seed)\n",
    "temperature = 0.75 #Primary addition, controls the temperature and our sampling behavior\n",
    "for i in tqdm(range(cur_len, sampling.size(1))):\n",
    "    with torch.no_grad():\n",
    "        h = autoReg_model(sampling[:,0:i])\n",
    "        h = h[:,-1,:] #Grab the last time step\n",
    "        h = F.softmax(h/temperature, dim=1) #make probabilities\n",
    "        next_tokens = torch.multinomial(h, 1)\n",
    "        sampling[:,i] = next_tokens\n",
    "\n",
    "        cur_len += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emilia:\n",
      "he supp\n",
      "and with the sport then good you know.\n",
      "\n",
      "pratester:\n",
      "now a whiles but me light'd heart;\n",
      "and the such would god polas, and a with hath peater: i have sfeory their you here thruafft commed this. in that prevery unlention stain make with my faurter:\n",
      "whouth entrans and simble,\n",
      "and whire thy dank\n",
      "thene woy.\n",
      "\n",
      "king secontion:\n",
      "ge wordd tarry three me as of as hath with thee him addveres, and stand then the fullaght and sight, he forethink when.\n",
      "\n",
      "piciness:\n",
      "a must to was boul partanous soule\n"
     ]
    }
   ],
   "source": [
    "s = [indx2vocab[x] for x in sampling.cpu().numpy().flatten()]\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:44<00:00, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emilia:\n",
      "the soul shall and the seem the sould to the soul the soul the soul the soul shall heart the prainter the seen the sould to the soul the soul the soul the soul the soul the soul the soul shall and the seen the soul the soul the sould to the soul the soul shall and the soul the soul and the soul the soul the soul the soul the soul the soul the soul the seen the soul the sould to the soul the soul the soul the soul the soul the soul the soul the soul the soul shall and the soul and the sou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cur_len = len(seed)\n",
    "temperature = 0.05 #Very low temp, will almost always pick the most likely items. \n",
    "for i in tqdm(range(cur_len, sampling.size(1))):\n",
    "    with torch.no_grad():\n",
    "        h = autoReg_model(sampling[:,0:i])\n",
    "        h = h[:,-1,:] #Grab the last time step\n",
    "        h = F.softmax(h/temperature, dim=1) #make probabilities\n",
    "        next_tokens = torch.multinomial(h, 1)\n",
    "        sampling[:,i] = next_tokens\n",
    "\n",
    "        cur_len += 1\n",
    "s = [indx2vocab[x] for x in sampling.cpu().numpy().flatten()]\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [00:00<00:00, 1098.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set up our seed and the location to store the generated content\n",
    "seed = \"EMILIA:\".lower()\n",
    "cur_len = len(seed)\n",
    "sampling = torch.zeros((1, 500), dtype=torch.int64, device=device)\n",
    "sampling[0,0:cur_len] = torch.tensor([vocab2indx[x] for x in seed])\n",
    "\n",
    "#pick a temperature\n",
    "temperature = 0.75\n",
    "with torch.no_grad():\n",
    "    #initialize the hidden state to avoid redundant work\n",
    "    h_prevs = autoReg_model.initHiddenStates(1)\n",
    "    #push the seed through\n",
    "    for i in range(0, cur_len):\n",
    "        h = autoReg_model.step(sampling[:,i], h_prevs=h_prevs)\n",
    "\n",
    "    #generate new text one character at a time\n",
    "    for i in tqdm(range(cur_len, sampling.size(1))):\n",
    "        h = F.softmax(h/temperature, dim=1) #make probabilities\n",
    "        next_tokens = torch.multinomial(h, 1)\n",
    "        sampling[:,i] = next_tokens\n",
    "        cur_len += 1\n",
    "        #now push only the new sample into the model\n",
    "        h = autoReg_model.step(sampling[:,i], h_prevs=h_prevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emilia:\n",
      "i have gracose we my lord, of the pears of thou in dead hamberoughs\n",
      "maled in eyeray, to know nower, we all be pitent queen in suffir and brach hearth be lught in offence.\n",
      "good my lord, sir, and made that are and the begnitlus had my husband, and i her.\n",
      "\n",
      "repralus:\n",
      "and her such dives this ampleader thy opwill be for well\n",
      "and the subter seent with see.\n",
      "\n",
      "valunioo:\n",
      "and the fellew their well now swear but lagy:\n",
      "if, if my ambarrower's their that soue thee\n",
      "fare and are my mound and hillods the c\n"
     ]
    }
   ],
   "source": [
    "s = [indx2vocab[x] for x in sampling.cpu().numpy().flatten()]\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
